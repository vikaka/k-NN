{
    "contents" : "---\ntitle: \"Assignment 2\"\nauthor: \"Vishesh Kakarala\"\ndate: \"February 29, 2016\"\noutput: html_document\n---\n```{r}\nlibrary('stats')\nlibrary('tseries')\nlibrary('forecast')\nlibrary('rms')\n```\n\n# Question 1\n\n## part A\n```{r}\njojo <- read.csv(\"jojo.csv\", quote = \" \", header = FALSE)\n\n# to remove seasonality from Time series data we use ts()\n\njojo_timeseries <- ts(jojo)\n\nacf(jojo_timeseries, main=\"ACF of time series\")\n```\n\nThe ACF plot show strong correlation, the auto correlations are very strong, positive and decay slowly\nNow we difference the Data  \n\n```{r}\njojo_diff <- diff(jojo_timeseries)\n\nplot(jojo_diff, main = \"Diff of time series\")\nabline(mean(jojo_diff),b=0)\n\nacf(jojo_diff,30,main = \"ACF of Diff time series\")\n\n\n```\n\nThe auto correlation function of the differenced data is plotted to check if stationarity is reduced.\n\nThe ACF decays to zero and hence difference of order one is sufficient for the model\n\nTest for Stationary time series\n\n```{r}\nBox.test(jojo_diff, type = \"Ljung-Box\")\n```\n\nThe p- value is <1 hence the function is stationary.\n\n## Question B\n\n### Now we analyze the stationary time series to examine the ACF and PACF\n\n\n```{r}\nacf(jojo_diff, xlab = \"1 lag = 4 months or 1 Quarter\",30, main = \"ACF analysis for MA order\")\n```\n\n```{r}\npacf(jojo_diff,xlab = \"1 lag = 4 months or 1 Quarter\", 30, main = \"ACF analysis for AR order\")\n```\n\nFrom the plots we can see that ACF shows a damping effect with significant spikes for 5 lag values, hence we can suggest an MA model of the order 5\n\nwhen we look at the PACF, it shows PACF with 3 significant lags hence a AR model of order 3 can be considered \n\nAn ARMA model of the order ARMA(3,5) where p = 3 and q = 5 can be implemented.\n\n## Question C\n\nfor ARIMA model specifiction\nThe time series is differenced once, hence d = 1\np = 3 & q= 5\n\n```{r}\njojo_arima <- arima(jojo_timeseries,order = c(3,1,5), method = \"ML\")\njojo_arima\n```\n\nHere p= 3, q= 3 and d = 1  \nWe can  try The arima estimates for different values,  \n\n```{r}\narima(jojo_timeseries,order = c(1,1,1), method = \"ML\")\narima(jojo_timeseries,order = c(0,1,2))\narima(jojo_timeseries,order = c(2,1,1))\n```\n\n## question D\n\n\nThe diagnostic plot of the fitted time series model is  \n\n```{r}\ntsdiag(jojo_arima)\n```\n\nThe p - values from the Ljung-Box test are all <1 hence the residuals are stationary\n\n## Question E\n\n```{r}\n#predicting values for ARIMA model wth parameters ARIMA(1,0,1)\n\njojo_fit_101 <- arima(jojo_timeseries,order = c(1,0,1), method = \"ML\")\npredicted_jojo <- predict(jojo_fit_101,11)\n\n#predicting values foe ARIMA model with parameters ARIMA(3,1,5) as deduced from the functions above\n\njojo_fit_315 <- arima(jojo_timeseries,order = c(3,1,5), method = \"ML\")\npredicted_jojo_315 <- predict(jojo_fit_315,11)\n\n\n```\n\n\n## Question F\n\n### Plotting the result with confidence intervals\n\n```{r}\n\n# plotting predicted values for ARIMA(1,0,1)\nplot(jojo_timeseries, xlim =c(1,100), ylim = c(0,30))\nlines(predicted_jojo$pred, col = \"blue\")\n\nlines(predicted_jojo$pred+predicted_jojo$se, col = \"red\")\nlines(predicted_jojo$pred-predicted_jojo$se, col = \"red\")\n\n#plotting predicted values for ARIMA(3,1,5)\nframe()\nplot(jojo_timeseries, xlim =c(1,100), ylim = c(0,30))\nlines(predicted_jojo_315$pred, col = \"blue\")\n\nlines(predicted_jojo_315$pred+predicted_jojo_315$se, col = \"red\")\nlines(predicted_jojo_315$pred-predicted_jojo_315$se, col = \"red\")\n\n\n```\n\n\n\n\n# Question 2\n\n\n```{r}\nlibrary('dplyr')\nlibrary('class')\n\nglass <- read.csv(\"~/Homework 2/glass.data\", header=FALSE)\nhead(glass)\nstr(glass)\ncolnames(glass) <- c(\"Id Number\",\"RI\",\"Na\",\"Mg\",\"Al\",\"Si\",\"K\",\"Ca\",\"Ba\",\"Fe\",\"Type of glass\")\n```\n## Question A\n\n```{r}\n# for loop to creat data frame column \"bi\"\n\nfor (i in 1:214)\n{\n  if(glass$'Type of glass'[i] <5)\n    {\n      glass$bi[i] <- 0\n    }\n  else\n    {\n    glass$bi[i] <- 1\n    }\n  i= i+1\n  \n}\n```\n\n## Question B\n```{r}\n\nfeatures <- glass[,-1]\nfeatures <- features[,-11]\nfeatures <- features[,-10]\n# to create the feature matrix the identifier,class type and \"bi\"\" columns are dropped\n\nfea <- data.frame(features)\n# y is the response vector\n\ny <- data.frame(glass$bi)\n```\n\n## Question C\n\nTraining - 80%  \nTesting - 20%  \nWe need a larger training data set to get a best fit model for prediction.    \n\n```{r}\n#sample is taken from the original data and the corresponding respone variables are also split\nset.seed(420)\n\ntrain_fea <- sample_frac(fea,0.8)\ntrain_y <- data.matrix(y[as.numeric(rownames(train_fea)),])\n\ntrain_values <- as.numeric(rownames(train_fea))\n\ntest_fea <- fea[-train_values,]\ntest_y <- data.matrix(y[as.numeric(rownames(test_fea)),])\n\n\ntrain_fea <- data.matrix(train_fea)\n\ntest_fea <- data.matrix(test_fea)\n\n#the training and test data is stored in matrix form\n```\n\n## Question D\n```{r}\n# From the class package, we use knn.cv to validate the exisitng response variables \n# kNN of order 5 is used\n\nfit_knn <- knn.cv(train_fea,train_y,k= 5)\n\n# the output is used as the new response varible and is used to predict test values\n```\n\n## Question E\n\n```{r}\n# Using the testing set and the training data predictions are made \n\ncheck_test <-data.frame(knn(train_fea,test_fea,fit_knn, k= 5))\ncheck_test$a <- test_y\n\ncolnames(check_test) <- c(\"Model output\",\"original data\")\n\n\ncheck_test$`Model output`<-data.frame(check_test$`Model output`)\n\nfor (i in 1:43)\n{\n  if(check_test$`Model output`[i,] == check_test$`original data`[i,])\n  {\n    check_test$accuracy[i] <- 1\n  }\n  else\n  {\n    check_test$accuracy[i] <- 0\n  }\n  i= i+1\n  \n}\n```\n**Accuracy of the kNN model of order 5 is **\n```{r}\n\n(sum(check_test$accuracy)/43)*100\n\n```\n\n## Question F\n\n```{r}\n\n#to compute the accuracy value for various levels of kNN\n# we must first initialise a storage data frame\n\naccuracy_test <- data.frame(5:25)\n\n\nfor(j in 1:20)\n{\n  check_test <-data.frame(knn(train_fea,test_fea,fit_knn, k= j+4))\n  check_test$a <- test_y\n\n  colnames(check_test) <- c(\"Model output\",\"original data\")\n\n  check_test$`Model output`<-data.frame(check_test$`Model output`)\n\n  for (i in 1:43)\n    {\n      if(check_test$`Model output`[i,] == check_test$`original data`[i,])\n        {\n          check_test$accuracy[i] <- 1\n        }\n      else\n        {\n          check_test$accuracy[i] <- 0\n        }\n    i= i+1\n  \n    }\n\n  accuracy_test$k_value[j] <- j+4\n  accuracy_test$Accuracy[j] <- sum(check_test$accuracy)/43\n  \n\n}\naccuracy_test <- accuracy_test[-1] \naccuracy_test <- accuracy_test[-21,] \n\n# the data frame accuracy_test contains the accuracy values for a reasonable range of K values\n```\n\n## Question G\n\n```{r}\nplot(accuracy_test$k_value,accuracy_test$Accuracy, type = \"l\")\n```\n\nOptimal value for k is below 10\n\n## Question H\n\nBy analysing the test class we can calculate the testing accuracy\n```{r}\ntable(test_y)\n```\nHere we can see that out of the 43 values how many 0's and 1's are there, hence to calculate null accuracy we get.  \n \n```{r}\n(as.vector(table(test_y))[1])/43*100\n```\n\n\n#Bonus Points\n\n# To determine which predictors are good, we can use linear modelling to determine their significance\n```{r}\nlm_multiple <- lm(glass$bi~glass$RI +glass$Na +glass$Mg+glass$Al+glass$Si+glass$K+glass$Ca+glass$Ba+glass$Fe+glass$`Type of glass`)\n\nsummary(lm_multiple)\n```\n# from the analysis of the summary we find that the varia bles Na, Al, SI and K have significant influence on the response vector\n\n```{r}\n\nfeatures <- glass[,-1]\nfeatures <- features[,-11]\nfeatures <- features[,-10]\nfeatures <- features[,-1]\nfeatures <- features[,-2]\nfeatures <- features[,-5]\nfeatures <- features[,-5]\nfeatures <- features[,-5]\n\ncolnames(features)\n# only the significant varibales are used in the feature matrix\n\nfea_sig <- data.frame(features)\n# y is the response vector\n\ny_sig <- data.frame(glass$bi)\n\n# normalizing the significant 4 variables for better kNN estimation\n\nfea_sig$Na <-(fea_sig$Na - min(fea_sig$Na))/(max(fea_sig$Na)-min(fea_sig$Na))\nfea_sig$Al <-(fea_sig$Al - min(fea_sig$Al))/(max(fea_sig$Al)-min(fea_sig$Al))\nfea_sig$Si <-(fea_sig$Si - min(fea_sig$Si))/(max(fea_sig$Si)-min(fea_sig$Si))\nfea_sig$K <-(fea_sig$K - min(fea_sig$K))/(max(fea_sig$K)-min(fea_sig$K))\n\n\n\n\n\n\n```\n\n#### The rest of the process is followed as above\n\n\n```{r}\n#sample is taken from the original data and the corresponding respone variables are also split\nset.seed(300)\n\ntrain_fea_sig <- sample_frac(fea_sig,0.8)\ntrain_y_sig <- data.matrix(y[as.numeric(rownames(train_fea_sig)),])\n\ntrain_values_sig <- as.numeric(rownames(train_fea_sig))\n\ntest_fea_sig <- fea_sig[-train_values_sig,]\ntest_y_sig <- data.matrix(y[as.numeric(rownames(test_fea_sig)),])\n\n\ntrain_fea_sig <- data.matrix(train_fea_sig)\n\ntest_fea_sig <- data.matrix(test_fea_sig)\n\n#the training and test data is stored in matrix form\n```\n\n\n```{r}\n# From the class package, we use knn.cv to validate the exisitng response variables \n# kNN of order 5 is used\n\nfit_knn_sig <- knn.cv(train_fea_sig,train_y_sig,k= 5)\n\n# the output is used as the new response varible and is used to predict test values\n```\n\n\n\n```{r}\n# Using the testing set and the training data predictions are made \n\ncheck_test_sig <-data.frame(knn(train_fea_sig,test_fea_sig,fit_knn_sig, k= 5))\ncheck_test_sig$a <- test_y_sig\n\ncolnames(check_test_sig) <- c(\"Model output\",\"original data\")\n\n\ncheck_test_sig$`Model output`<-data.frame(check_test_sig$`Model output`)\n\nfor (i in 1:43)\n{\n  if(check_test_sig$`Model output`[i,] == check_test_sig$`original data`[i,])\n  {\n    check_test_sig$accuracy[i] <- 1\n  }\n  else\n  {\n    check_test_sig$accuracy[i] <- 0\n  }\n  i= i+1\n  \n}\n```\n**accuracy of the kNN model of order 5 is **\n```{r}\n(sum(check_test_sig$accuracy)/43)*100\n\n\n\n```\n\n\n\n```{r}\n\n#to compute the accuracy value for various levels of kNN\n# we must first initialise a storage data frame\n\naccuracy_test_4 <- data.frame(5:25)\n\n\nfor(j in 1:20)\n{\n  check_test_sig <-data.frame(knn(train_fea_sig,test_fea_sig,fit_knn_sig, k= j+4))\n  check_test_sig$a <- test_y\n\n  colnames(check_test_sig) <- c(\"Model output\",\"original data\")\n\n  check_test_sig$`Model output`<-data.frame(check_test_sig$`Model output`)\n\n  for (i in 1:43)\n    {\n      if(check_test_sig$`Model output`[i,] == check_test_sig$`original data`[i,])\n        {\n          check_test_sig$accuracy[i] <- 1\n        }\n      else\n        {\n          check_test_sig$accuracy[i] <- 0\n        }\n    i= i+1\n  \n    }\n\n  accuracy_test_4$k_value[j] <- j+4\n  accuracy_test_4$Accuracy[j] <- sum(check_test_sig$accuracy)/43\n  \n\n}\naccuracy_test_4 <- accuracy_test_4[-1] \naccuracy_test_4 <- accuracy_test_4[-21,] \n\n# the data frame accuracy_test_4 contains the accuracy values for a reasonable range of K values\n```\n\n\n```{r}\nplot(accuracy_test_4$k_value,accuracy_test_4$Accuracy, type = \"l\")\n```\n\nOptimal value for k is between 5 and 8\n\n\nBy analysing the test class we can calculate the testing accuracy\n```{r}\ntable(test_y_sig)\n```\nHere we can see that out of the 43 values how many 0's and 1's are there, hence to calculate null accuracy we get.  \n\n```{r}\n (as.vector(table(test_y_sig))[1]/43)*100\n```\n\n\n",
    "created" : 1456766775861.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "3829847368",
    "id" : "352EF1C0",
    "lastKnownWriteTime" : 1459276594,
    "path" : "~/Homework 2/Assignment 2 - Vishesh Kakarala.Rmd",
    "project_path" : "Assignment 2 - Vishesh Kakarala.Rmd",
    "properties" : {
        "tempName" : "Untitled1"
    },
    "relative_order" : 7,
    "source_on_save" : false,
    "type" : "r_markdown"
}